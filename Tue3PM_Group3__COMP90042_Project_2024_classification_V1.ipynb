{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "In this part, we need some json files that we prepared from previous evidence retrieval part. Please ensure that you put them into the right directory.\n",
        "In details, they are:\n",
        "1. \"dev_concatenated_claim_evidences.json\"\n",
        "2. \"test_concatenated_claim_evidences.json\"\n",
        "3. \"pred_train_wrongly_pred_evidences.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVOq0AIFdzVu"
      },
      "source": [
        "**We use keras, tensorflow, nltk, scikit-learn in this project.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the dataset from github"
      ],
      "metadata": {
        "id": "e35nqDIhr3rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# the repository link:\n",
        "repository_url = 'https://github.com/drcarenhan/COMP90042_2024.git'\n",
        "\n",
        "# clone the repository\n",
        "os.system(f'git clone {repository_url}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TssQWt03lYsO",
        "outputId": "68a7e7f6-7bc4-4c2d-e657-ec6c2991f771"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/COMP90042_2024/data'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "output_file_path = os.path.join(save_path, 'evidence.json')\n",
        "\n",
        "!gdown --id '1JlUzRufknsHzKzvrEjgw8D3n_IRpjzo6' -O {output_file_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm3ApdnXlaI4",
        "outputId": "cbe09022-b9cf-4b76-aadd-a4cc3eddea5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1JlUzRufknsHzKzvrEjgw8D3n_IRpjzo6\n",
            "From (redirected): https://drive.google.com/uc?id=1JlUzRufknsHzKzvrEjgw8D3n_IRpjzo6&confirm=t&uuid=8669b27c-c7e5-45fc-93d4-ea64c695be49\n",
            "To: /content/COMP90042_2024/data/evidence.json\n",
            "100% 174M/174M [00:05<00:00, 32.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/COMP90042_2024/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYvEaxd9lb3P",
        "outputId": "21251e87-6194-436e-ad60-c857dc87709d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/COMP90042_2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvff21Hv8zjk",
        "tags": []
      },
      "source": [
        "## 1.1 PreProcess for evidence and claims"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code of stemming, lemmatizing and stopword removal are referred from tutorial."
      ],
      "metadata": {
        "id": "imiwmjYHsA_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data files\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stopwords_set = set(stopwords.words('english'))\n",
        "\n",
        "# Lemmatizer function\n",
        "def lemmatize(word):\n",
        "    lemma = lemmatizer.lemmatize(word, 'v')\n",
        "    if lemma == word:\n",
        "        lemma = lemmatizer.lemmatize(word, 'n')\n",
        "    return lemma\n",
        "\n",
        "# Text preprocessing function\n",
        "def text_preprocessing(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenizing\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Lemmatizing and removing stopwords\n",
        "    new_words = [lemmatize(w) for w in words if w not in stopwords_set]\n",
        "\n",
        "    return \" \".join(new_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGujQLQJlhD6",
        "outputId": "61f89f27-ec9a-44fe-bfab-6203bbcf482a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcfiY-axdzVv"
      },
      "source": [
        "### 1.1.1 read files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auxilary functions for reading and pre-processing the data."
      ],
      "metadata": {
        "id": "_yk3vZ95sK8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def process_claims(claims, evidences_id_dict):\n",
        "    \"\"\"\n",
        "    Process claims data to extract relevant information and map evidence IDs.\n",
        "\n",
        "    Args:\n",
        "    claims (dict): A dictionary of claims where each key is a claim ID and each value is a dictionary containing claim details.\n",
        "    evidences_id_dict (dict): A dictionary mapping evidence IDs to indices for quick access.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Contains lists of claim IDs, claim texts, preprocessed claim texts, associated evidence indices, and claim labels.\n",
        "    \"\"\"\n",
        "    ids = []\n",
        "    texts = []\n",
        "    processed_texts = []\n",
        "    evidences = []\n",
        "    labels = []\n",
        "\n",
        "    for claim_id, data in claims.items():\n",
        "        ids.append(claim_id)\n",
        "        texts.append(data[\"claim_text\"])\n",
        "        processed_texts.append(text_preprocessing(data[\"claim_text\"]))\n",
        "        labels.append(data.get(\"claim_label\", None))  # Test data may not have labels.\n",
        "        evidences.append([evidences_id_dict[i] for i in data.get(\"evidences\", [])])\n",
        "\n",
        "    return ids, texts, processed_texts, evidences, labels\n",
        "\n",
        "def process_evidences(evidences):\n",
        "    \"\"\"\n",
        "    Process evidences data to extract relevant information and create a mapping from evidence IDs to indices.\n",
        "\n",
        "    Args:\n",
        "    evidences (dict): A dictionary of evidences where each key is an evidence ID and the value is the evidence text.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Contains lists of evidence IDs, evidence texts, preprocessed evidence texts, and a dictionary mapping IDs to indices.\n",
        "    \"\"\"\n",
        "    ids = []\n",
        "    texts = []\n",
        "    processed_texts = []\n",
        "    id_dict = {}\n",
        "\n",
        "    for idx, (evidence_id, evidence_text) in enumerate(evidences.items()):\n",
        "        ids.append(evidence_id)\n",
        "        texts.append(evidence_text)\n",
        "        processed_texts.append(text_preprocessing(evidence_text))\n",
        "        id_dict[evidence_id] = idx\n",
        "\n",
        "    return ids, texts, processed_texts, id_dict"
      ],
      "metadata": {
        "id": "ytqKNFtYlld4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the functions to read the data."
      ],
      "metadata": {
        "id": "x94Ic2tEsN5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from files\n",
        "with open(save_path+'/train-claims.json', 'r') as file:\n",
        "    train_claims = json.load(file)\n",
        "\n",
        "with open(save_path+'/evidence.json', 'r') as file:\n",
        "    evidences = json.load(file)\n",
        "\n",
        "with open(save_path+'/dev-claims.json', 'r') as file:\n",
        "    dev_claims = json.load(file)\n",
        "\n",
        "with open(save_path+'/test-claims-unlabelled.json', 'r') as file:\n",
        "    test_claims = json.load(file)\n",
        "\n",
        "# Process evidence data to prepare for linkage with claims\n",
        "evidences_ids, evidences_texts, evidences_p_texts, evidences_id_dict = process_evidences(evidences)\n",
        "\n",
        "# Process claims data for training, development, and test sets using the evidence dictionary\n",
        "train_ids, train_claim_texts, train_p_claim_texts, train_evidences, train_labels = process_claims(train_claims, evidences_id_dict)\n",
        "dev_ids, dev_claim_texts, dev_p_claim_texts, dev_evidences, dev_labels = process_claims(dev_claims, evidences_id_dict)\n",
        "test_ids, test_claim_texts, test_p_claim_texts, _, _ = process_claims(test_claims, evidences_id_dict)"
      ],
      "metadata": {
        "id": "ZNcRVaKPloLD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the data that we prepared in the evidence retrieval part."
      ],
      "metadata": {
        "id": "kNa_AiW7tvwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "dev_concatenated_claim_evidences = json.load(open(\"dev_concatenated_claim_evidences.json\", \"r\"))\n",
        "test_concatenated_claim_evidences = json.load(open(\"test_concatenated_claim_evidences.json\", \"r\"))\n",
        "\n",
        "train_wrongly_pred_evidences = json.load(open(\"pred_train_wrongly_pred_evidences.json\", \"r\"))"
      ],
      "metadata": {
        "id": "OeX_L3XDtvFH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "parse the files from previous evidence retrieval part to get the concatenated input and label for dev and test dataset."
      ],
      "metadata": {
        "id": "97wG7ds6HKf_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dfhnKhhQdzVy"
      },
      "outputs": [],
      "source": [
        "dev_inputs = [i['text'] for i in dev_concatenated_claim_evidences]\n",
        "test_inputs = [i['text'] for i in test_concatenated_claim_evidences]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.2 Construct vocab and indexing"
      ],
      "metadata": {
        "id": "3e-2Xf9htmoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(texts, min_count=3):\n",
        "    \"\"\"\n",
        "    Build a vocabulary from a list of texts, filtering words by a minimum count threshold.\n",
        "\n",
        "    Args:\n",
        "    texts (list of str): A list of sentences from which to build the vocabulary.\n",
        "    min_count (int): Minimum occurrence threshold for words to be included in the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Two dictionaries - idx2word (maps index to word) and word2idx (maps word to index).\n",
        "    \"\"\"\n",
        "    # Initialize word count dictionary and predefined special tokens.\n",
        "    wordcount = {}\n",
        "    idx2word = [\"<pad>\", \"<cls>\", \"<sep>\", \"<unk>\"]\n",
        "    word2idx = {\"<pad>\": 0, \"<cls>\": 1, \"<sep>\": 2, \"<unk>\": 3}\n",
        "\n",
        "    # Count occurrences of each word in the texts.\n",
        "    for text in texts:\n",
        "        for word in text.split():\n",
        "            wordcount[word] = wordcount.get(word, 0) + 1\n",
        "\n",
        "    # Start indexing for new words from 4 since 0-3 are reserved for special tokens.\n",
        "    idx = len(idx2word)\n",
        "\n",
        "    # Include words in the vocabulary only if they meet the minimum count criteria.\n",
        "    for word, count in wordcount.items():\n",
        "        if count > min_count:\n",
        "            idx2word.append(word)\n",
        "            word2idx[word] = idx\n",
        "            idx += 1\n",
        "\n",
        "    return idx2word, word2idx\n",
        "\n",
        "# Use the function to build the vocabulary from training and evidence texts.\n",
        "idx2word, word2idx = build_vocabulary(train_claim_texts + evidences_texts, min_count=3)"
      ],
      "metadata": {
        "id": "59wjGPUml6E-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_indices(text_data, word2idx):\n",
        "    \"\"\"\n",
        "    Convert a list of sentences into lists of indices based on a given word-to-index mapping.\n",
        "\n",
        "    Args:\n",
        "    text_data (list of str): A list of sentences to be converted.\n",
        "    word2idx (dict): A dictionary mapping words to their corresponding indices.\n",
        "\n",
        "    Returns:\n",
        "    list of list of int: A list where each sentence is represented as a list of indices.\n",
        "    \"\"\"\n",
        "    # Initialize the list that will store the converted sentences.\n",
        "    idx_data = []\n",
        "\n",
        "    # Iterate over each sentence in the input list.\n",
        "    for text in text_data:\n",
        "        # Convert each word in the sentence to its corresponding index.\n",
        "        # If the word is not found in the dictionary, use the index for \"<unk>\".\n",
        "        indices = [word2idx.get(word, word2idx[\"<unk>\"]) for word in text.split()]\n",
        "\n",
        "        # Append the list of indices to the main list.\n",
        "        idx_data.append(indices)\n",
        "\n",
        "    return idx_data"
      ],
      "metadata": {
        "id": "ubyhsH__l8e8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_claim_text_idx = convert_to_indices(train_claim_texts, word2idx)\n",
        "dev_claim_text_idx = convert_to_indices(dev_claim_texts, word2idx)\n",
        "test_claim_text_idx = convert_to_indices(test_claim_texts, word2idx)\n",
        "evidences_text_idx = convert_to_indices(evidences_texts, word2idx)"
      ],
      "metadata": {
        "id": "qE-L5m41l-TO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the length for padding and truncating and prepare the label for classification. The value of length is chosen by considering the statistics of length in previous evidence retrieval part.\n",
        "\n",
        "Train - Average: 20.09771986970684, Median: 19.0,Max: 67\n",
        "\n",
        "\n",
        "Dev - Average: 21.084415584415584, Median: 18.0,Max: 65\n",
        "\n",
        "\n",
        "Test - Average: 20.03921568627451, Median: 19.0,Max: 53\n",
        "\n",
        "\n",
        "Evidence - Average: 19.691925312720514, Median: 18.0,Max: 479"
      ],
      "metadata": {
        "id": "uhYCnJIw1xyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the length for our model input\n",
        "# limit the claim to be at most 60\n",
        "claim_max_len = 60\n",
        "# limit the evidence to be at most 100\n",
        "evidence_max_len = 100\n",
        "# limit the max length for the concatenated claim and evidences\n",
        "# claim length + evidence length * 5 + special tokens * 6\n",
        "# 60 + 100*5 + 6 = 566\n",
        "concatenated_max_len = 570\n",
        "retrieval_num = 5\n",
        "\n",
        "# prepare the label and corresponding index\n",
        "# Transform the string label into index can make the traning more efficient\n",
        "id2labels = [\"SUPPORTS\", \"NOT_ENOUGH_INFO\", \"REFUTES\", \"DISPUTED\"]\n",
        "labels2id = {\"SUPPORTS\": 0, \"NOT_ENOUGH_INFO\": 1, \"REFUTES\": 2, \"DISPUTED\": 3}"
      ],
      "metadata": {
        "id": "N1OI1Vcy1w03"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform the string label into index number for convenient training."
      ],
      "metadata": {
        "id": "lJJStTMsN3r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_labels = [labels2id[i[\"label\"]] for i in dev_concatenated_claim_evidences]\n",
        "train_labels = [labels2id[i] for i in train_labels]"
      ],
      "metadata": {
        "id": "qDjFjdFiJTdk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Construct the dataloader"
      ],
      "metadata": {
        "id": "9eE7THMS4iUj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "S4ZxqS9kdzVx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class that handles data involving claims and their associated evidences.\n",
        "\n",
        "    Args:\n",
        "    claim_data (list): List of claim text indices.\n",
        "    evidence_data (list): List of evidence text indices.\n",
        "    true_evidences (list): List of indices pointing to true evidence for each claim.\n",
        "    wrongly_retrieved_evidences (list): List of indices of incorrectly retrieved evidences.\n",
        "    label (list): List of labels corresponding to the claim data.\n",
        "    cls_idx (int): Index used for <cls> token.\n",
        "    sep_idx (int): Index used for <sep> token.\n",
        "    pad_idx (int): Index used for <pad> token.\n",
        "    evidence_num (int): Number of evidences to use per claim.\n",
        "    \"\"\"\n",
        "    def __init__(self, claim_data, evidence_data, true_evidences, wrongly_retrieved_evidences, labels, cls_token, sep_token, pad_token, evidence_num=5):\n",
        "        self.claim_data = claim_data\n",
        "        self.evidence_data = evidence_data\n",
        "        self.true_evidences = true_evidences\n",
        "        self.wrongly_retrieved_evidences = wrongly_retrieved_evidences\n",
        "        self.labels = labels\n",
        "        self.cls_token = cls_token\n",
        "        self.sep_token = sep_token\n",
        "        self.pad_token = pad_token\n",
        "        self.evidence_num = evidence_num\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of items in the dataset.\"\"\"\n",
        "        return len(self.claim_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "        list: Data for a single training example including claim, evidences, and label.\n",
        "        \"\"\"\n",
        "        return [self.claim_data[idx][:claim_max_len], self.true_evidences[idx], self.wrongly_retrieved_evidences[idx], self.labels[idx]]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"\n",
        "        Custom collate function to process the batch, used by DataLoader to prepare batches.\n",
        "\n",
        "        Args:\n",
        "        batch (list): List of elements returned by __getitem__.\n",
        "\n",
        "        Returns:\n",
        "        dict: Dictionary containing tensors of queries, positions, and labels.\n",
        "        \"\"\"\n",
        "        queries, queries_pos, batch_labels = [], [], []\n",
        "\n",
        "        for claim, true_evid, wrong_evid, label in batch:\n",
        "            concatenated_text = self.construct_query_text(claim, true_evid, wrong_evid)\n",
        "            queries.append(concatenated_text)\n",
        "            queries_pos.append(list(range(len(concatenated_text))))\n",
        "            batch_labels.append(label)\n",
        "\n",
        "        return {\n",
        "            \"queries\": torch.LongTensor(queries),\n",
        "            \"queries_pos\": torch.LongTensor(queries_pos),\n",
        "            \"labels\": torch.LongTensor(batch_labels)\n",
        "        }\n",
        "\n",
        "    def construct_query_text(self, claim, true_evid, wrong_evid):\n",
        "        \"\"\"\n",
        "        Construct the full text for a query by concatenating claim and evidence texts with special tokens.\n",
        "\n",
        "        Args:\n",
        "        claim (list): List of indices for claim text.\n",
        "        true_evid (list): Indices of true evidence.\n",
        "        wrong_evid (list): Indices of wrong evidence.\n",
        "\n",
        "        Returns:\n",
        "        list: Combined list of indices including special tokens and padded to maximum length.\n",
        "        \"\"\"\n",
        "        # The whole query start with the claim and cls token\n",
        "        concatenated_text = [self.cls_token] + claim\n",
        "        # compute the number of wrongly retrieved evidences needed to add to meet the evidence_num\n",
        "        evidences_to_include = self.evidence_num - len(true_evid)\n",
        "\n",
        "        all_evidences = true_evid + random.sample(wrong_evid, evidences_to_include)\n",
        "        # concatenate the evidence to the claim, seperated by sep token\n",
        "        for evid_idx in all_evidences:\n",
        "            concatenated_text += [self.sep_token] + self.evidence_data[evid_idx][:evidence_max_len]\n",
        "\n",
        "        concatenated_text.append(self.sep_token)\n",
        "\n",
        "        # if the length of concatenated text is still shorter than the given max length, pad it with padding token\n",
        "        if len(concatenated_text) < concatenated_max_len:\n",
        "            concatenated_text.extend([self.pad_token] * (concatenated_max_len - len(concatenated_text)))\n",
        "\n",
        "        return concatenated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0jD1GjlIdzVy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_set = TrainDataset(train_claim_text_idx, evidences_text_idx, train_evidences, train_wrongly_pred_evidences, train_labels,\n",
        "                         word2idx[\"<cls>\"], word2idx[\"<sep>\"], word2idx[\"<pad>\"], evidence_num=retrieval_num)\n",
        "dataloader = DataLoader(train_set, batch_size=10, shuffle=True, num_workers=1, collate_fn=train_set.collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2.Model Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define our transformer based classifier. The code of this part is referred from workshops."
      ],
      "metadata": {
        "id": "pdac_QJbURvU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "scrolled": true,
        "id": "Xk-mIDjYdzVz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CLS(nn.Module):\n",
        "    \"\"\"\n",
        "    A neural network module for classification tasks using a transformer encoder.\n",
        "\n",
        "    Args:\n",
        "    vocab_size (int): Size of the vocabulary.\n",
        "    embed_dim (int): Dimensionality of the embeddings.\n",
        "    hidden_size (int): Size of the hidden layer.\n",
        "    output_size (int): The size of the output layer, which corresponds to the number of classes.\n",
        "    nhead (int): Number of heads in the multi-head attention models of the transformer.\n",
        "    num_layers (int): Number of transformer layers to stack.\n",
        "    max_position (int): Maximum sequence length that can be processed by this model.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size, output_size, nhead, num_layers, max_position):\n",
        "        super(CLS, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_position, embed_dim)\n",
        "\n",
        "        # Initialize transformer encoder layer and the encoder itself\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=nhead, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers, norm=nn.LayerNorm(hidden_size))\n",
        "\n",
        "        # Hidden layer that reduces dimensionality from hidden_size to hidden_size // 2\n",
        "        self.hidden_layer = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        # Output layer for classification\n",
        "        self.pred_layer = nn.Linear(hidden_size // 2, output_size)\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, text_data, position_text):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "        text_data (Tensor): Indices of words in the batch, shape (batch_size, seq_length)\n",
        "        position_text (Tensor): Positional indices corresponding to `text_data`.\n",
        "\n",
        "        Returns:\n",
        "        Tensor: Output from the final classification layer.\n",
        "        \"\"\"\n",
        "        # Create mask for padding tokens\n",
        "        mask_ = text_data == 0\n",
        "\n",
        "        # Combine word embeddings with position embeddings scaled down by 0.01\n",
        "        text_embeddings = self.embedding(text_data)\n",
        "        position_embeddings = self.pos_embedding(position_text) * 0.01\n",
        "        encoder_text_input = text_embeddings + position_embeddings\n",
        "\n",
        "        # Apply the transformer encoder\n",
        "        text_encoder_output = self.encoder(encoder_text_input, src_key_padding_mask=mask_)\n",
        "\n",
        "        # Apply the first linear transformation and non-linearity\n",
        "        encoder_output_cls = text_encoder_output[:, 0, :]  # we use the first token of encoder output for classification purposes\n",
        "        hidden_output = F.relu(self.hidden_layer(encoder_output_cls))\n",
        "        hidden_output = self.dropout(hidden_output)\n",
        "\n",
        "        # Final classification layer\n",
        "        prediction = self.pred_layer(hidden_output)\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6If740RddzVz",
        "outputId": "a9dcf31b-c530-49d4-9e5e-13af7f92459b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLS(\n",
              "  (embedding): Embedding(197728, 512)\n",
              "  (pos_embedding): Embedding(700, 512)\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-4): 5 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (hidden_layer): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (pred_layer): Linear(in_features=256, out_features=4, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "cls_model = CLS(vocab_size=len(idx2word), embed_dim=512, hidden_size=512, output_size=4, nhead=8, num_layers=5, max_position=700)\n",
        "cls_model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In1A2YaxdzV0"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Set the related parameters before training"
      ],
      "metadata": {
        "id": "pygqE2cwelLk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cAKrTd1AdzV0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import random\n",
        "torch.manual_seed(90042)\n",
        "torch.cuda.manual_seed_all(90042)\n",
        "random.seed(90042)\n",
        "\n",
        "encoder_optimizer = optim.Adam(cls_model.parameters())\n",
        "max_lr = 1e-3\n",
        "for param_group in encoder_optimizer.param_groups:\n",
        "    param_group['lr'] = max_lr\n",
        "\n",
        "save_dir = \"model_ckpts\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Define auxilary functions used in training"
      ],
      "metadata": {
        "id": "yIHe-IuWpUlW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GlUceDH6dzV0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def validate(dev_input, dev_labels, cls_model):\n",
        "    \"\"\"\n",
        "    Validate the classification model's performance on the development set.\n",
        "\n",
        "    Args:\n",
        "    dev_input (list of list of int): The input data for validation, where each item is a sequence of token indices.\n",
        "    dev_output (list of int): The ground truth output labels for the validation data.\n",
        "    cls_model (torch.nn.Module): The classification model to validate.\n",
        "\n",
        "    Returns:\n",
        "    float: The accuracy of the model on the development data.\n",
        "    \"\"\"\n",
        "    cls_model.eval()  # Switch the model to evaluation mode\n",
        "    batch_size = 50\n",
        "    total_correct = 0\n",
        "    total_count = len(dev_labels)\n",
        "\n",
        "    # Process the dataset in batches\n",
        "    for start_idx in range(0, total_count, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, total_count)\n",
        "        batch_input = torch.LongTensor(dev_input[start_idx:end_idx]).cuda()\n",
        "        batch_pos = torch.LongTensor([list(range(len(dev_input[0]))) for _ in range(end_idx - start_idx)]).cuda()\n",
        "\n",
        "        # Perform prediction\n",
        "        batch_output = cls_model(batch_input, batch_pos)\n",
        "        # Pick the predicted label that has the highest probability among 4 labels\n",
        "        predicted_labels = torch.argmax(batch_output, axis=1).cpu().tolist()\n",
        "\n",
        "        # Compute accuracy for the batch\n",
        "        total_correct += sum(1 for predicted, true in zip(predicted_labels, dev_labels[start_idx:end_idx]) if predicted == true)\n",
        "\n",
        "        # Free CUDA memory\n",
        "        del batch_input, batch_pos\n",
        "\n",
        "    # Calculate total accuracy\n",
        "    accuracy = total_correct / total_count\n",
        "    print(f\"\\nClassification Accuracy: {accuracy:.3f}\\n\")\n",
        "\n",
        "    cls_model.train()  # Switch back to training mode\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "scrolled": true,
        "id": "Ed4A_lxLdzV0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(cls_model, dataloader, epochs, encoder_optimizer, loss_function, grad_norm, accumulate_step, warmup_steps, max_lr, eval_interval, save_dir):\n",
        "    \"\"\"\n",
        "    Train a classification model.\n",
        "\n",
        "    Args:\n",
        "    cls_model (torch.nn.Module): The classification model to be trained.\n",
        "    dataloader (DataLoader): DataLoader for training data.\n",
        "    encoder_optimizer (Optimizer): Optimizer for the model.\n",
        "    loss_function: Used for computing loss\n",
        "    grad_norm (float): Maximum norm for gradient clipping.\n",
        "    accumulate_step (int): Number of steps to accumulate gradients before backward pass.\n",
        "    warmup_steps (int): Number of steps for linear learning rate warmup.\n",
        "    max_lr (float): Maximum learning rate after warmup.\n",
        "    eval_interval (int): Interval for evaluating the model.\n",
        "    save_dir (str): Directory to save the best model checkpoint.\n",
        "    \"\"\"\n",
        "    step_cnt = 0\n",
        "    all_step_cnt = 0\n",
        "    avg_loss = 0\n",
        "    maximum_f_score = 0\n",
        "\n",
        "    for epoch in range(epochs):  # Iterate over epochs\n",
        "        epoch_step = 0\n",
        "\n",
        "        for i, batch in enumerate(tqdm(dataloader, desc=\"Training Epoch {}\".format(epoch+1))):\n",
        "            step_cnt += 1\n",
        "\n",
        "            # Forward pass\n",
        "            cur_res = cls_model(batch[\"queries\"].cuda(), batch[\"queries_pos\"].cuda())\n",
        "            loss = loss_function(cur_res, batch[\"labels\"].cuda()) / accumulate_step\n",
        "            loss.backward()\n",
        "\n",
        "            # Accumulate loss for reporting\n",
        "            avg_loss += loss.item()\n",
        "\n",
        "            # Parameter update\n",
        "            if step_cnt == accumulate_step:\n",
        "                if grad_norm > 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(cls_model.parameters(), grad_norm)\n",
        "\n",
        "                encoder_optimizer.step()\n",
        "                encoder_optimizer.zero_grad()\n",
        "                step_cnt = 0\n",
        "                epoch_step += 1\n",
        "                all_step_cnt += 1\n",
        "\n",
        "                # Adjust learning rate\n",
        "                lr = adjust_learning_rate(encoder_optimizer, all_step_cnt, warmup_steps, max_lr)\n",
        "\n",
        "                # Report training status\n",
        "                if all_step_cnt % report_freq == 0:\n",
        "                    print(f\"Epoch: {epoch + 1}, Step: {epoch_step}, Avg Loss: {avg_loss / report_freq:.6f}, Learning Rate: {lr:.6f}\")\n",
        "                    avg_loss = 0  # Reset average loss\n",
        "\n",
        "            # Free up memory\n",
        "            del loss, cur_res\n",
        "\n",
        "            # Periodic evaluation and checkpointing\n",
        "            if all_step_cnt % eval_interval == 0 and all_step_cnt != 0:\n",
        "                f_score = validate(dev_inputs, dev_labels, cls_model)  # Evaluate on development set\n",
        "                # save the best checkpoint to avoid overfitting\n",
        "                if f_score > maximum_f_score:\n",
        "                    maximum_f_score = f_score\n",
        "                    torch.save(cls_model.state_dict(), os.path.join(save_dir, \"best_cls_ckpt.bin\"))\n",
        "                    print(f\"New best F-score: {f_score:.4f} at Epoch: {epoch + 1}, Step: {epoch_step}\")\n",
        "\n",
        "def adjust_learning_rate(optimizer, step_cnt, warmup_steps, max_lr):\n",
        "    \"\"\"\n",
        "    Adjusts learning rate based on step count.\n",
        "\n",
        "    Args:\n",
        "    optimizer (Optimizer): Optimizer whose learning rate needs adjustment.\n",
        "    step_cnt (int): Current step count.\n",
        "    warmup_steps (int): Steps to linearly increase learning rate.\n",
        "    max_lr (float): Target learning rate post-warmup.\n",
        "\n",
        "    Returns:\n",
        "    float: Adjusted learning rate.\n",
        "    \"\"\"\n",
        "    if step_cnt <= warmup_steps:\n",
        "        lr = step_cnt * (max_lr - 2e-8) / warmup_steps + 2e-8\n",
        "    else:\n",
        "        lr = max_lr - (step_cnt - warmup_steps) * 1e-6\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    return lr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set some hyperparameters for training."
      ],
      "metadata": {
        "id": "w5y-8zFmrRZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accumulate_step = 2\n",
        "grad_norm = 5\n",
        "warmup_steps = 250\n",
        "report_freq = 15\n",
        "eval_interval = 40\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "XRwGrinQp31A"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform a statistics on the distribution of training dataset labels here."
      ],
      "metadata": {
        "id": "Fh4Xpnc6RXCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print(Counter(train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqvAyEObsHEp",
        "outputId": "880eee5b-0ba4-4d54-b146-b1b10e391d43"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({0: 519, 1: 386, 2: 199, 3: 124})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we can set the different weights for different labels when computing the crossentropy loss as the labels are imbalanced in the training dataset.\n",
        "\n",
        "\n",
        "{'SUPPORTS': 519, 'NOT_ENOUGH_INFO': 386, 'REFUTES': 199, 'DISPUTED': 124}"
      ],
      "metadata": {
        "id": "4ijI6BZgrNyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For more common label like 'SUPPORTS', we give it smaller weight as 0.2.\n",
        "# For minority label like 'DISPUTED' and 'REFUTES', we give larger weight.\n",
        "# These weights are tunable hyperparameters, too\n",
        "labels_weights = torch.FloatTensor([0.3, 0.4, 0.7, 1.]).cuda()\n",
        "loss_function = nn.CrossEntropyLoss(labels_weights)"
      ],
      "metadata": {
        "id": "OWFGJFGzrNCV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Training start here!"
      ],
      "metadata": {
        "id": "M7sL7ol0sTWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(cls_model, dataloader,epochs, encoder_optimizer, loss_function, grad_norm, accumulate_step, warmup_steps, max_lr, eval_interval, save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LZLUEnKpzey",
        "outputId": "95a51a88-cb90-4f57-8cb6-22a2763775b2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1:  24%|██▍       | 30/123 [00:07<00:23,  3.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Step: 15, Avg Loss: 1.408939, Learning Rate: 0.000060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1:  49%|████▉     | 60/123 [00:15<00:16,  3.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Step: 30, Avg Loss: 1.384404, Learning Rate: 0.000120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1:  64%|██████▍   | 79/123 [00:20<00:11,  3.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Accuracy: 0.266\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Epoch 1:  65%|██████▌   | 80/123 [00:28<01:52,  2.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best F-score: 0.2662 at Epoch: 1, Step: 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Epoch 1:  66%|██████▌   | 81/123 [00:30<01:34,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Accuracy: 0.266\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1:  73%|███████▎  | 90/123 [00:32<00:11,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Step: 45, Avg Loss: 1.413497, Learning Rate: 0.000180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1:  98%|█████████▊| 120/123 [00:40<00:00,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Step: 60, Avg Loss: 1.389134, Learning Rate: 0.000240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 123/123 [00:41<00:00,  2.98it/s]\n",
            "Training Epoch 2:  22%|██▏       | 27/123 [00:07<00:25,  3.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Step: 14, Avg Loss: 1.437560, Learning Rate: 0.000300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2:  30%|███       | 37/123 [00:10<00:51,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Accuracy: 0.175\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Epoch 2:  31%|███       | 38/123 [00:12<01:09,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Accuracy: 0.175\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2:  46%|████▋     | 57/123 [00:17<00:17,  3.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Step: 29, Avg Loss: 1.389128, Learning Rate: 0.000360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2:  71%|███████   | 87/123 [00:25<00:09,  3.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Step: 44, Avg Loss: 1.335564, Learning Rate: 0.000420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2:  94%|█████████▍| 116/123 [00:32<00:01,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Step: 59, Avg Loss: 1.472062, Learning Rate: 0.000480\n",
            "\n",
            "Classification Accuracy: 0.442\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Epoch 2:  95%|█████████▌| 117/123 [00:40<00:14,  2.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best F-score: 0.4416 at Epoch: 2, Step: 59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Epoch 2:  96%|█████████▌| 118/123 [00:41<00:10,  2.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Accuracy: 0.442\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 123/123 [00:43<00:00,  2.84it/s]\n",
            "Training Epoch 3:  20%|█▉        | 24/123 [00:06<00:26,  3.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Step: 12, Avg Loss: 1.405230, Learning Rate: 0.000540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3:  44%|████▍     | 54/123 [00:14<00:18,  3.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Step: 27, Avg Loss: 1.420614, Learning Rate: 0.000600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3:  60%|██████    | 74/123 [00:20<00:29,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Accuracy: 0.266\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Epoch 3:  61%|██████    | 75/123 [00:22<00:39,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Accuracy: 0.266\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3:  68%|██████▊   | 84/123 [00:24<00:11,  3.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Step: 42, Avg Loss: 1.377736, Learning Rate: 0.000660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3:  93%|█████████▎| 114/123 [00:32<00:02,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Step: 57, Avg Loss: 1.396883, Learning Rate: 0.000720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 123/123 [00:35<00:00,  3.51it/s]\n",
            "Training Epoch 4:  17%|█▋        | 21/123 [00:05<00:26,  3.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Step: 11, Avg Loss: 1.377941, Learning Rate: 0.000780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4:  25%|██▌       | 31/123 [00:09<00:54,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Accuracy: 0.442\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Epoch 4:  26%|██▌       | 32/123 [00:10<01:14,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Accuracy: 0.442\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4:  41%|████▏     | 51/123 [00:15<00:19,  3.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Step: 26, Avg Loss: 1.363693, Learning Rate: 0.000840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4:  66%|██████▌   | 81/123 [00:23<00:11,  3.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Step: 41, Avg Loss: 1.409509, Learning Rate: 0.000900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4:  89%|████████▉ | 110/123 [00:31<00:03,  3.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Step: 56, Avg Loss: 1.388116, Learning Rate: 0.000960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Epoch 4:  90%|█████████ | 111/123 [00:32<00:07,  1.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Accuracy: 0.442\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Epoch 4:  91%|█████████ | 112/123 [00:34<00:08,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Accuracy: 0.442\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 123/123 [00:36<00:00,  3.33it/s]\n",
            "Training Epoch 5:  15%|█▍        | 18/123 [00:04<00:27,  3.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Step: 9, Avg Loss: 1.381740, Learning Rate: 0.000995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5:  39%|███▉      | 48/123 [00:12<00:19,  3.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Step: 24, Avg Loss: 1.383381, Learning Rate: 0.000980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5:  55%|█████▌    | 68/123 [00:19<00:32,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Accuracy: 0.266\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Epoch 5:  56%|█████▌    | 69/123 [00:20<00:43,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Accuracy: 0.266\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5:  63%|██████▎   | 78/123 [00:22<00:12,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Step: 39, Avg Loss: 1.380678, Learning Rate: 0.000965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5:  88%|████████▊ | 108/123 [00:30<00:03,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Step: 54, Avg Loss: 1.378711, Learning Rate: 0.000950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 123/123 [00:34<00:00,  3.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the function for prediction on the given input"
      ],
      "metadata": {
        "id": "9h7-5LHWDhwe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "jCvlubN1dzV1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def predict(input, cls_model):\n",
        "    \"\"\"\n",
        "    Predict claim labels for a given set of concatenated inputs using the trained classification model.\n",
        "\n",
        "    Args:\n",
        "    input (list of list of int): The data inputs, where each input is a list of token indices.\n",
        "    cls_model (torch.nn.Module): The classification model to use for prediction.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of predicted class labels.\n",
        "    \"\"\"\n",
        "    cls_model.eval()  # Set the model to evaluation mode to disable dropout, etc.\n",
        "    batch_size = 75\n",
        "    pos_len = len(input[0])  # Assume all inputs are the same length\n",
        "    predictions = []\n",
        "\n",
        "    # Iterate through the input in batches\n",
        "    for start_idx in range(0, len(input), batch_size):\n",
        "        end_idx = min(start_idx + batch_size, len(input))\n",
        "\n",
        "        # Prepare batch data for model input\n",
        "        batch_input = torch.LongTensor(input[start_idx:end_idx]).view(-1, pos_len).cuda()\n",
        "        batch_pos = torch.LongTensor([list(range(pos_len)) for _ in range(end_idx - start_idx)]).cuda()\n",
        "\n",
        "        # Get the model predictions for the current batch\n",
        "        batch_res = cls_model(batch_input, batch_pos)\n",
        "        predicted_labels = torch.argmax(batch_res, 1).tolist()  # pick out the label that has the highest probability\n",
        "\n",
        "        # Collect all predictions\n",
        "        predictions.extend(predicted_labels)\n",
        "\n",
        "        # Clean up memory\n",
        "        del batch_input, batch_pos\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the prediction"
      ],
      "metadata": {
        "id": "j_1DFr9vDoEw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "6OEuu09idzV1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "cls_model.load_state_dict(torch.load(os.path.join(save_dir, \"best_cls_ckpt.bin\")))\n",
        "\n",
        "dev_predicted_labels = predict(dev_inputs, cls_model)\n",
        "test_predicted_labels = predict(test_inputs, cls_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store the predicted labels into the dictionary with the retrieved evidences together."
      ],
      "metadata": {
        "id": "PPoDm5YkDtvp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "mlZsn2RmdzV1"
      },
      "outputs": [],
      "source": [
        "pred_dev_claims = json.load(open(\"pred_dev_claims_retrieval.json\", \"r\"))\n",
        "pred_test_claims = json.load(open(\"pred_test_claims_retrieval.json\", \"r\"))\n",
        "\n",
        "def update_claim_labels(claim_ids, predicted_labels, claims_dict, id_to_label):\n",
        "    \"\"\"\n",
        "    Update claim dictionaries with predicted labels translated from label IDs.\n",
        "\n",
        "    Args:\n",
        "    claim_ids (list): List of claim identifiers.\n",
        "    predicted_labels (list): List of predicted label IDs corresponding to the claim IDs.\n",
        "    claims_dict (dict): Dictionary of claims where each key is a claim ID and the value is the claim data.\n",
        "    id_to_label (dict): Dictionary mapping label IDs to human-readable labels.\n",
        "\n",
        "    Effect:\n",
        "    Modifies the claims_dict by adding a 'claim_label' field with the translated label.\n",
        "    \"\"\"\n",
        "    for claim_id, label_id in zip(claim_ids, predicted_labels):\n",
        "        if claim_id in claims_dict:\n",
        "            claims_dict[claim_id]['claim_label'] = id_to_label[label_id]\n",
        "        else:\n",
        "            print(f\"Warning: Claim ID {claim_id} not found in claims dictionary.\")\n",
        "\n",
        "update_claim_labels(dev_ids, dev_predicted_labels, pred_dev_claims, id2labels)\n",
        "update_claim_labels(test_ids, test_predicted_labels, pred_test_claims, id2labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "oezHqkWjdzV2"
      },
      "outputs": [],
      "source": [
        "## save the final predicted test data for leaderboard submission\n",
        "json.dump(pred_test_claims, open(\"test-output.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test_claims"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZONg1m_PN9N",
        "outputId": "59282825-7053-4ca7-af90-95277db48276"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'claim-2967': {'claim_text': 'The contribution of waste heat to the global climate is 0.028 W/m2.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-3',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-979': {'claim_text': '“Warm weather worsened the most recent five-year drought, which included the driest four-year period on record in terms of statewide precipitation.',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1609': {'claim_text': 'Greenland has only lost a tiny fraction of its ice mass.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1020': {'claim_text': '“The global reef crisis does not necessarily mean extinction for coral species.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2599': {'claim_text': 'Small amounts of very active substances can cause large effects.',\n",
              "  'evidences': ['evidence-4',\n",
              "   'evidence-6',\n",
              "   'evidence-8',\n",
              "   'evidence-0',\n",
              "   'evidence-1'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2110': {'claim_text': \"They changed the name from 'global warming' to 'climate change'\",\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-5',\n",
              "   'evidence-6'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1135': {'claim_text': 'Scientists confirm a mass bleaching event on the Great Barrier Reef this year has killed more corals than ever before, with more than two thirds destroyed across large swathes of the biodiverse site.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-712': {'claim_text': '“Instead of a three-foot increase in ocean levels by the end of the century, six feet was more likely, according to DeConto and Pollard’s findings.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1307': {'claim_text': 'a new study by scientists at the Australian Research Council finds that the ongoing bleaching event is mainly due to human-caused global warming, and that if global warming proceeds as currently expected, “large parts” of the Great Barrier Reef could die by the mid-2030s.',\n",
              "  'evidences': ['evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5',\n",
              "   'evidence-6'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-148': {'claim_text': 'Previous IPCC reports tended to assume that clouds would have a neutral impact because the warming and cooling feedbacks would cancel each other out.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-903': {'claim_text': 'Growers regularly pump CO2 into greenhouses, raising levels to three times that of the natural environment, to produce stronger, greener, healthier plants.”',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-0',\n",
              "   'evidence-2'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2942': {'claim_text': 'It’s usually taken to be the fact that as carbon dioxide concentrations in the atmosphere increase, the 1 per cent of CO2 that’s the heavier carbon isotope ratio c13 declines in proportion.',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-5',\n",
              "   'evidence-8',\n",
              "   'evidence-0',\n",
              "   'evidence-2'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1001': {'claim_text': '‘This study goes beyond statistical correlations and explores a specific process that can plausibly explain how enhanced high-latitude warming trends may trigger remote weather impacts,’ he said.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-7',\n",
              "   'evidence-1',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1034': {'claim_text': 'Your odds of correctly guessing the outcome of a flipped coin are 1 in 2, but your odds of guessing correctly twice in a row are only 1 in 4 —',\n",
              "  'evidences': ['evidence-3',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1009': {'claim_text': '‘Arctic ice conditions have been tracking at record low conditions since October, persisting for six consecutive months’',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-770': {'claim_text': '[CO2] has increased 43 percent above the pre-industrial level so far',\n",
              "  'evidences': ['evidence-4',\n",
              "   'evidence-7',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-3074': {'claim_text': 'Fourth Assessment Report (AR4), in which the likely range is given as 2.0°C to 4.5°C, with a best estimate of 3.0°C\" (Pat Michaels)',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1761': {'claim_text': 'Multiple lines of evidence make it very clear that the rise in atmospheric CO2 is due to human emissions.',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1475': {'claim_text': \"More than 500 scientists and professionals in climate and related fields have sent a 'European Climate Declaration' to the Secretary-General of the United Nations.\",\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-477': {'claim_text': 'Scientists say halting deforestation ‘just as urgent’ as reducing emissions',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-4',\n",
              "   'evidence-5',\n",
              "   'evidence-6'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1378': {'claim_text': 'Antarctica is gaining land-based ice, according to a new study by NASA scientists published in the Journal of Glaciology',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-503': {'claim_text': '“During the sunless winter, a heatwave raised concerns that the polar vortex may be eroding.',\n",
              "  'evidences': ['evidence-8',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2751': {'claim_text': 'The argument that solving the global warming problem by reducing human greenhouse gas emissions is \"too hard\" generally stems from the belief that (i) our technology is not sufficiently advanced to achieve significant emissions reductions,',\n",
              "  'evidences': ['evidence-7',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2575': {'claim_text': 'Over the last 30-40 years 80% of coral in the Caribbean have been destroyed and 50% in Indonesia and the Pacific.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-30': {'claim_text': \"the bushfires [in Australia] were caused by arsonists and a series of lightning strikes, not 'climate change'\",\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2994': {'claim_text': \"'Mauna Loa has been producing a readout which supports Manning's predetermined goal by showing steady growth in atmospheric CO2 concentrations since 1959.\",\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-55': {'claim_text': '[T]he raw data, the actual thermometer data[...] shows that the US has been cooling for 80 to 90 years.',\n",
              "  'evidences': ['evidence-5',\n",
              "   'evidence-6',\n",
              "   'evidence-8',\n",
              "   'evidence-9',\n",
              "   'evidence-0'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1271': {'claim_text': 'Greenland ice sheet … would balloon sea levels by around 7m should it disintegrate',\n",
              "  'evidences': ['evidence-2',\n",
              "   'evidence-7',\n",
              "   'evidence-9',\n",
              "   'evidence-0',\n",
              "   'evidence-1'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2248': {'claim_text': 'Climate sensitivity can be calculated empirically by comparing past temperature change to natural forcings at the time.',\n",
              "  'evidences': ['evidence-6',\n",
              "   'evidence-7',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-532': {'claim_text': 'Never mind that the emissions of carbon dioxide to make and maintain a wind or solar industrial complex are far greater than they will ever save.',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-5',\n",
              "   'evidence-8',\n",
              "   'evidence-0'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-556': {'claim_text': 'The April low temperatures here are now about 6 degrees Fahrenheit warmer than they used to be.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1173': {'claim_text': '[…]until we develop a practical, cost-competitive alternative to fossil fuels, it is unlikely that renewable energy will ever make up more than 15-20% of global energy requirements.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-539': {'claim_text': 'We note that the Earth has never in its history had a quasi-stable state that is around 2C warmer than the preindustrial and suggest that there is substantial risk that the system, itself, will ‘want’ to continue warming because of all of these other processes – even if we stop emissions,” she said.',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-3',\n",
              "   'evidence-5',\n",
              "   'evidence-6',\n",
              "   'evidence-0'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-893': {'claim_text': 'While France and other G7 countries lamented the U.S. exit from the Paris climate accord, America’s air is already cleaner than that of any other country in the G7, except Canada with its scant population.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-8',\n",
              "   'evidence-3'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2857': {'claim_text': 'Weather is chaotic because air is light, it has low friction and viscosity, it expands strongly when in contact with hot surfaces and it conducts heat poorly.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-4',\n",
              "   'evidence-5',\n",
              "   'evidence-6',\n",
              "   'evidence-7'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-109': {'claim_text': 'Local and regional sea levels continue to exhibit typical natural variability—in some places rising and in others falling.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2476': {'claim_text': \"The global dimming trend reversed around 1990 - 15 years after the global warming trend began in the mid 1970's.\",\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-3038': {'claim_text': 'Clearly, other factors besides atmospheric carbon influence earth temperatures and global warming.\"',\n",
              "  'evidences': ['evidence-5',\n",
              "   'evidence-6',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-3127': {'claim_text': '\"Skeptics hope that Postma’s alternative thermal model will lead  to the birth of a new climatology, one that actually follows the laws of  physics and properly physical modeling techniques...',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-474': {'claim_text': 'Sea ice continued its declining trend, both in the Arctic and Antarctic.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2464': {'claim_text': 'While natural forcings can account for much of the early 20th Century warming, humans played a role as well.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2427': {'claim_text': 'By that time CO2 emissions had already risen from the expanded use  of coal that had powered the industrial revolution, and emissions only  increased slowly from 3.5gigatonnes in 1910 to under 4gigatonnes by the  end of the Second World War.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2167': {'claim_text': 'Royal Society embraces skepticism',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-7'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-812': {'claim_text': 'The human contribution can be up to 30 percent or so of the total rainfall coming out of the storm',\n",
              "  'evidences': ['evidence-6',\n",
              "   'evidence-7',\n",
              "   'evidence-8',\n",
              "   'evidence-9',\n",
              "   'evidence-0'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2590': {'claim_text': \"This argument originates from Angstrom's work in 1901.\",\n",
              "  'evidences': ['evidence-2',\n",
              "   'evidence-7',\n",
              "   'evidence-8',\n",
              "   'evidence-9',\n",
              "   'evidence-0'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-404': {'claim_text': 'Actual weather records over the past 100 years show no correlation between rising carbon dioxide levels and local temperatures.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2977': {'claim_text': 'According to the study, as carbon levels have risen, the cold air at high altitudes over the tropics has actually grown colder.',\n",
              "  'evidences': ['evidence-7',\n",
              "   'evidence-8',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2673': {'claim_text': 'They changed the name from “global warming” to “climate change” after the term global warming just wasn’t working (it was too cold)!',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2509': {'claim_text': 'In 1905, PDO switched to a warm phase.',\n",
              "  'evidences': ['evidence-9',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-138': {'claim_text': 'As a result, half of the people surveyed around the world last year said they thought climate change would make humanity extinct.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-952': {'claim_text': 'The Alaskan tundra is warming so quickly it has become a net emitter of carbon dioxide ahead of schedule, a new study finds',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-5',\n",
              "   'evidence-6'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1691': {'claim_text': 'Sea level rise is now increasing faster than predicted due to unexpectedly rapid ice melting.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1741': {'claim_text': 'Volcanoes have had no warming effect in recent global warming - if anything, a cooling effect.',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5',\n",
              "   'evidence-6'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1202': {'claim_text': '\\xa0The discrepancy between model-predicted warming and (lower) real-world observations has inspired new respect for natural climate variability relative to greenhouse-gas forcing.\\xa0',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1028': {'claim_text': 'A second coat of paint has much less of an effect, while adding a third or fourth coat has almost no impact at all.”',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-28': {'claim_text': 'Volcanoes Melting West Antarctic Glaciers, Not Global Warming',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-5',\n",
              "   'evidence-6',\n",
              "   'evidence-7'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-275': {'claim_text': 'until temperature increases began to slow down after 1998 and remained relatively stable for a period of 15 years',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-1',\n",
              "   'evidence-2'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-350': {'claim_text': 'Since 1965, more parts of the U.S. have seen a decrease in flooding than have seen an increase.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2204': {'claim_text': 'All the indicators show that global warming is still happening.',\n",
              "  'evidences': ['evidence-6',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1604': {'claim_text': 'CO2 increase is natural, not human-caused.',\n",
              "  'evidences': ['evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-6',\n",
              "   'evidence-9',\n",
              "   'evidence-0'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-3119': {'claim_text': 'CO2 constitutes 80% of the non-condensing greenhouse gas forcing.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2150': {'claim_text': 'Adapting to global warming is cheaper than preventing it',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-21': {'claim_text': 'Sea level rise has been slow and a constant, pre-dating industrialization',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-7',\n",
              "   'evidence-8',\n",
              "   'evidence-1'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2013': {'claim_text': 'U.S. Sen. Ron Johnson voted to let oil and gas companies emit \"unlimited carbon pollution into our air\"',\n",
              "  'evidences': ['evidence-5',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-467': {'claim_text': 'This is the South Pole ice, 90% of Earth’s ice, and it’s getting thicker.',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-3',\n",
              "   'evidence-5',\n",
              "   'evidence-6',\n",
              "   'evidence-7'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2754': {'claim_text': '\"There are many urgent priorities that need the attention of Congress, and it is not for me as an invited guest in your country to say what they are.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2797': {'claim_text': 'Global brightening is caused by changes in cloud cover, reflective aerosols and absorbing aerosols.',\n",
              "  'evidences': ['evidence-7',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1771': {'claim_text': 'Peer-reviewed research, physics, and math all tell us that a grand solar minimum would have no more than a 0.3°C cooling effect, barely enough to put a dent in human-caused global warming.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1908': {'claim_text': 'In reality, gas produced by fracking is worse for the climate than coal.',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2000': {'claim_text': 'There is not a single candidate in the Republican primary that thinks we should do anything about climate change.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2084': {'claim_text': \"Hurricanes aren't linked to global warming\",\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-6'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1237': {'claim_text': 'the last sea-level high point, … occurred between the last two ice ages, about 125,000 years ago.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-400': {'claim_text': 'The idea that climate change is producing heat records across the Earth is among the most egregious manipulations of data in the absurd global warming debate.',\n",
              "  'evidences': ['evidence-6',\n",
              "   'evidence-8',\n",
              "   'evidence-9',\n",
              "   'evidence-0',\n",
              "   'evidence-1'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1508': {'claim_text': 'global warming ceased around the end of the twentieth century and was followed (since 1997) by 19 years of stable temperature.',\n",
              "  'evidences': ['evidence-3',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-520': {'claim_text': 'Just 1.25 per cent of the carbon dioxide in the atmosphere-ocean system has been released by \\xadhumans in the past 250 years.',\n",
              "  'evidences': ['evidence-7',\n",
              "   'evidence-9',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-3064': {'claim_text': 'No warming since at least 1995, no melting glaciers and now no rising sea levels.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-5',\n",
              "   'evidence-6',\n",
              "   'evidence-8'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1588': {'claim_text': \"Melting ice isn't warming the Arctic.\",\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-6',\n",
              "   'evidence-7'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1488': {'claim_text': 'New Study Confirms EVs Considerably Worse For Climate Than Diesel Cars.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-4',\n",
              "   'evidence-5',\n",
              "   'evidence-6'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2733': {'claim_text': 'A number of independent studies using near-global satellite data find positive feedback and high climate sensitivity.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-5',\n",
              "   'evidence-6'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-809': {'claim_text': 'In the early 20th century, state and federal governments began aggressively fighting wildfires and trying to keep them as small as possible.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-763': {'claim_text': 'And some plants – like algae which use carbon for photosynthesis – may even benefit.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-454': {'claim_text': 'Man-made greenhouse gases play only an insignificant role.”',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1853': {'claim_text': 'Barack Obama supports proposals \"to devote billions of dollars annually to state game and fish agencies and federal land management agencies to help them ensure that fish and wildlife survive the impacts of climate change.\"',\n",
              "  'evidences': ['evidence-5',\n",
              "   'evidence-7',\n",
              "   'evidence-8',\n",
              "   'evidence-0',\n",
              "   'evidence-1'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2838': {'claim_text': \"The data is being reported by the University of Illinois's Arctic Climate Research Center, and is derived from satellite observations of the Northern and Southern hemisphere polar regions (Daily Tech).\",\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2028': {'claim_text': 'cutting speed limits could slow climate change',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2434': {'claim_text': 'The IPCC blames human emissions of carbon dioxide for the last  warming.',\n",
              "  'evidences': ['evidence-6',\n",
              "   'evidence-7',\n",
              "   'evidence-8',\n",
              "   'evidence-0',\n",
              "   'evidence-1'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-298': {'claim_text': 'Only very few peer-reviewed papers even go so far as to say that recent warming is chiefly anthropogenic.',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-0',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-338': {'claim_text': 'Rapid assessment of average temperatures in France between 26-28 June showed a “substantial” increase in the likelihood of the heatwave happening as a result of human-caused global warming, experts at the World Weather Attribution group said.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-6'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1672': {'claim_text': 'Global temperature is still rising and 2010 was the hottest recorded.',\n",
              "  'evidences': ['evidence-7',\n",
              "   'evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2840': {'claim_text': 'Arctic sea ice is also falling at an accelerated rate.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1425': {'claim_text': 'Each year sees the disappearance of thousands of plant and animal species which we will never know, which our children will never see, because they have been lost for ever.',\n",
              "  'evidences': ['evidence-5',\n",
              "   'evidence-6',\n",
              "   'evidence-7',\n",
              "   'evidence-9',\n",
              "   'evidence-0'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1985': {'claim_text': 'The North Pole is melting \"a bit\" but the South Pole is getting bigger.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1156': {'claim_text': 'The costs of emissions regulations, which will be paid by everyone, will be punishingly high and will provide no benefits to most people anywhere in the world.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2870': {'claim_text': \"Ironically, it's those who are mispresenting Hulme's paper that are the ones being misleading.\",\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2898': {'claim_text': 'The report suggests significantly smaller overall  ice-mass losses than previous estimates.',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-5',\n",
              "   'evidence-7'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2329': {'claim_text': 'We know the Northwest Passage had been open before.\"',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-4',\n",
              "   'evidence-5',\n",
              "   'evidence-6'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1998': {'claim_text': 'There is a link between climate change and the NSW bushfires.',\n",
              "  'evidences': ['evidence-2',\n",
              "   'evidence-4',\n",
              "   'evidence-5',\n",
              "   'evidence-9',\n",
              "   'evidence-1'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2209': {'claim_text': 'Surface temperatures can show short-term cooling when heat is exchanged between the atmosphere and the ocean, which has a much greater heat capacity than the air.',\n",
              "  'evidences': ['evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5',\n",
              "   'evidence-8',\n",
              "   'evidence-1'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1582': {'claim_text': \"Tuvalu sea level isn't rising.\",\n",
              "  'evidences': ['evidence-7',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-3072': {'claim_text': '\"a paper...in Science magazine concludes that the climate sensitivity—how much the earth’s average temperature will rise as a result of a doubling of the atmospheric concentration of carbon dioxide—likely (that is, with a 66% probability) lies in the range 1.7°C to 2.6°C, with a median value of 2.3°C.',\n",
              "  'evidences': ['evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-381': {'claim_text': '[subsidies for wind and solar] add to emissions because coal-fired elec\\xadtricity needs to be on standby for when there is no wind or sunshine.',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-3',\n",
              "   'evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-398': {'claim_text': 'nothing we can do to stop the Earth’s naturally occurring climate cycles.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1560': {'claim_text': 'Sea level rise predictions are exaggerated.',\n",
              "  'evidences': ['evidence-7',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2246': {'claim_text': '\"His [Dr Spencer\\'s] latest research demonstrates that – in the short term, at any rate –  the temperature feedbacks that the IPCC imagines will greatly amplify  any initial warming caused by CO2 are net-negative, attenuating the  warming they are supposed to enhance.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2774': {'claim_text': 'Actual reconstructions \"diverge\" from the instrumental series in the last part of 20th century.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-7'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-972': {'claim_text': '“Arctic land stores about twice as much carbon as the atmosphere.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-3',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1531': {'claim_text': 'Earth’s climate is now changing faster than at any point in the history of modern civilization, primarily as a result of human activities.',\n",
              "  'evidences': ['evidence-2',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2592': {'claim_text': '\"A July 6, 2007 study published in the journal Science about Greenland by an international team of scientists found DNA “evidence that suggests the frozen shield covering the immense island survived the Earth’s last period of global warming,” according to a Boston Globe Article. \\xa0...',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2468': {'claim_text': '\"Satellite measurements indicate an absence of significant global warming since 1979, the very period that human carbon dioxide emissions have been increasing rapidly.',\n",
              "  'evidences': ['evidence-6',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-463': {'claim_text': '“So now we’re able to explain from natural factors how we’ve had the 20th Century warming.',\n",
              "  'evidences': ['evidence-6',\n",
              "   'evidence-8',\n",
              "   'evidence-9',\n",
              "   'evidence-0',\n",
              "   'evidence-1'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-616': {'claim_text': 'The effect of long-term warming is to make it harder to count on snowmelt runoff in wet times',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1240': {'claim_text': '“Ice-free means the central basin of the Arctic will be ice-free',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-4',\n",
              "   'evidence-3'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2951': {'claim_text': 'The two most cited composites are PMOD and ACRIM.',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-3',\n",
              "   'evidence-6',\n",
              "   'evidence-7',\n",
              "   'evidence-8'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1977': {'claim_text': 'The climate-change agreement between the United States and China \"requires the Chinese to do nothing at all for 16 years.\"',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-9',\n",
              "   'evidence-0',\n",
              "   'evidence-3'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-942': {'claim_text': '‘We could be decades too fast, or decades too slow,’said one of them, Robert M. DeConto of the University of Massachusetts, Amherst.',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-4',\n",
              "   'evidence-6',\n",
              "   'evidence-7',\n",
              "   'evidence-8'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2755': {'claim_text': 'Global warming is an increasingly urgent problem.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1230': {'claim_text': 'On the Pacific Coast, a climate pattern that had pushed billions of gallons of water toward Asia is now ending, so that in coming decades the sea is likely to rise quickly off states like Oregon and California.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-3123': {'claim_text': 'Given that a doubling of carbon dioxide would change the surface heat flux by only two watts per square meter, it is evident that a small change in cloud cover can strongly affect the response to carbon dioxide.\"',\n",
              "  'evidences': ['evidence-5',\n",
              "   'evidence-7',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1684': {'claim_text': 'The 2nd law of thermodynamics is consistent with the greenhouse effect which is directly observed.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-839': {'claim_text': '“Even if we meet the Paris goals of two degrees warming, cities like Karachi and Kolkata will become close to uninhabitable, annually encountering deadly heat waves like those that crippled them in 2015.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2423': {'claim_text': 'Arctic sea ice has been steadily thinning, even in the last few years while the surface ice (eg - sea ice extent) increased slightly.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1243': {'claim_text': 'by that I mean the central Arctic will be ice-free.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-494': {'claim_text': 'The Massachusetts Bay Colony in 1635 experienced a Category 3 or 4 storm, with up to a 20-foot storm surge.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1458': {'claim_text': 'Glaciers are retreating almost everywhere around the world — including in the Alps, Himalayas, Andes, Rockies, Alaska and Africa.',\n",
              "  'evidences': ['evidence-4',\n",
              "   'evidence-7',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-461': {'claim_text': 'Temperatures cooled from about 1940 to 1975, and then they rose from about ’75 to about 2005 or so, and since then they’ve been flat or cooling.',\n",
              "  'evidences': ['evidence-4',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1304': {'claim_text': 'in a letter to The Times from Lord Krebs and company, essentially telling the newspaper to stop reporting less-than-negative climate stories.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-8',\n",
              "   'evidence-3'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2564': {'claim_text': 'This small warming is  likely a result of the natural alterations in global ocean currents  which are driven by ocean salinity variations.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2121': {'claim_text': 'Infrared Iris will reduce global warming',\n",
              "  'evidences': ['evidence-4',\n",
              "   'evidence-5',\n",
              "   'evidence-6',\n",
              "   'evidence-8',\n",
              "   'evidence-9'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2631': {'claim_text': 'The temperatures are expected to change by as much as 10 Fahrenheit degrees at different places of the globe.',\n",
              "  'evidences': ['evidence-3',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1141': {'claim_text': '“While record low sea ice is nothing new in the Arctic, this is a surprising turn of events for the Antarctic.',\n",
              "  'evidences': ['evidence-3',\n",
              "   'evidence-5',\n",
              "   'evidence-7',\n",
              "   'evidence-8',\n",
              "   'evidence-9'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2398': {'claim_text': 'While there are direct ways in which CO2 is a pollutant (acidification of the ocean), its primary impact is its greenhouse warming effect.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1048': {'claim_text': 'While transient weather variability is playing a key role here, the widespread record warmth across the U.S. so far this year is part of a long-term trend toward more warm temperature records versus cold ones.',\n",
              "  'evidences': ['evidence-5',\n",
              "   'evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2783': {'claim_text': 'The study indicates far less future global warming will occur than United Nations computer models have predicted, and supports prior studies indicating increases in atmospheric carbon dioxide trap far less heat than alarmists have claimed.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1003': {'claim_text': 'A recent study in Nature Geoscience, for instance, called into question whether the Arctic’s melting, and in particular its sea ice loss, has been causing winter cooling over Eurasia, another idea that has been swept up in the debate over the jet stream and weather extremes.”',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1872': {'claim_text': 'Donald Trump claims Global Warming is a hoax',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5',\n",
              "   'evidence-8'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1842': {'claim_text': 'A video shows Koko the gorilla spontaneously using sign language to issue a warning about climate change.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2411': {'claim_text': 'Note that computer models are just concatenations of calculations you could do on a hand-held calculator, so they are theoretical and cannot be part of any evidence.\"',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2428': {'claim_text': 'It was the post war  industrialization that caused the rapid rise in global CO2 emissions,  but by 1945 when this began, the Earth was already in a cooling phase  that started around 1942 and continued until 1975.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1198': {'claim_text': '” ‘You see, gas in America is incredibly cheap, because of fracking,’ he says.',\n",
              "  'evidences': ['evidence-1',\n",
              "   'evidence-4',\n",
              "   'evidence-5',\n",
              "   'evidence-7',\n",
              "   'evidence-9'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-678': {'claim_text': 'In the process, the cows will emit much greenhouse gas, and they will consume far more calories in beans than they will yield in meat, meaning far more clearcutting of forests to farm cattle feed than would be necessary if the beans above were simply eaten by people.”',\n",
              "  'evidences': ['evidence-2',\n",
              "   'evidence-4',\n",
              "   'evidence-7',\n",
              "   'evidence-9',\n",
              "   'evidence-0'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2105': {'claim_text': 'Excess CO2 from human emissions has a long residence time of over 100 years',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-648': {'claim_text': '‘there will not be enough nitrogen available to sustain the high carbon uptake scenarios.’',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-822': {'claim_text': 'Climate policy must compete with other long-term threats for always-scarce resources.”',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2561': {'claim_text': 'Observed sea levels are actually tracking at the upper range of the IPCC projections.',\n",
              "  'evidences': ['evidence-8',\n",
              "   'evidence-3',\n",
              "   'evidence-5',\n",
              "   'evidence-6',\n",
              "   'evidence-7'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2219': {'claim_text': 'The increase in temperatures since 1975 is a consistent feature of all reconstructions.',\n",
              "  'evidences': ['evidence-5',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-8'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1343': {'claim_text': 'roughly three-quarters of the tidal flood days now occurring in towns along the East Coast would not be happening in the absence of the rise in the sea level caused by human emissions.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5',\n",
              "   'evidence-7'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1351': {'claim_text': 'a marginally significant warming trend in the data over the past several years, erasing the temperature plateau that vexed climate alarmists have found difficult to explain.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-3',\n",
              "   'evidence-4',\n",
              "   'evidence-5',\n",
              "   'evidence-1'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2347': {'claim_text': 'In truth, the overwhelming majority of climate-research funding comes from the federal government and left-wing foundations.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-4',\n",
              "   'evidence-5',\n",
              "   'evidence-6',\n",
              "   'evidence-1'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-293': {'claim_text': 'When the measuring equipment gets old and needs replacing, it often requires re-calibration.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-910': {'claim_text': 'The cement, iron and steel, and petroleum refining industries could see their production cut by 21% 19%, and 11% respectively.”',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-2815': {'claim_text': 'A new peer-reviewed study on Surface Warming and the Solar Cycle found that times of high solar activity are on average 0.2°C warmer than times of low solar activity, and that there is a polar amplification of the warming.',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-2',\n",
              "   'evidence-3',\n",
              "   'evidence-4'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1652': {'claim_text': 'The strong CO2 effect has been observed by many different measurements.',\n",
              "  'evidences': ['evidence-5',\n",
              "   'evidence-6',\n",
              "   'evidence-7',\n",
              "   'evidence-9',\n",
              "   'evidence-0'],\n",
              "  'claim_label': 'SUPPORTS'},\n",
              " 'claim-1212': {'claim_text': '(In technical lingo, the so-called social cost of carbon would be negative.)”',\n",
              "  'evidences': ['evidence-0',\n",
              "   'evidence-1',\n",
              "   'evidence-3',\n",
              "   'evidence-6',\n",
              "   'evidence-7'],\n",
              "  'claim_label': 'SUPPORTS'}}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP",
        "tags": []
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}